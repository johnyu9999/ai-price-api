from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import List
import joblib
import numpy as np
import time
import logging
import os
from sklearn.linear_model import LinearRegression
import uuid
from functools import lru_cache
from collections import defaultdict
import time

MODEL_VERSION = "v1.0-fallback"

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®­ç»ƒä¸€ä¸ªæ¨¡å‹
model_path = "model.pkl"

rate_limit = defaultdict(list)
MAX_REQUESTS = 5
WINDOW_SECONDS = 60

if not os.path.exists(model_path):
    logging.info("ğŸš§ model.pkl not found. Training a simple fallback model...")
    np.random.seed(42)
    X = np.random.rand(100, 3)
    weights = np.array([1.5, -2.0, 3.0])
    y = X @ weights + 4.2 + np.random.randn(100) * 0.1
    model = LinearRegression()
    model.fit(X, y)
    joblib.dump(model, model_path)
    logging.info("âœ… Fallback model trained and saved to model.pkl")
else:
    model = joblib.load(model_path)
    logging.info("âœ… model.pkl loaded.")

# å®šä¹‰è¾“å…¥æ ¼å¼
class PredictRequest(BaseModel):
    features: List[float]

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI()

@app.get("/")
def read_root():
    return {"status": "ok", "message": "API is live."}

@app.get("/healthz")
def health_check():
    return {"status": "healthy"}

@lru_cache(maxsize=128)
def cached_predict(feature_tuple):
    X = np.array([feature_tuple])
    return model.predict(X)[0]

@app.post("/predict")
async def predict(request: Request, body: PredictRequest):
ip = request.client.host
now = time.time()

# æ¸…é™¤è¿‡æœŸæ—¶é—´æˆ³
rate_limit[ip] = [t for t in rate_limit[ip] if now - t < WINDOW_SECONDS]

if len(rate_limit[ip]) >= MAX_REQUESTS:
    raise HTTPException(status_code=429, detail="Too many requests - rate limit exceeded.")

# è®°å½•å½“å‰è¯·æ±‚æ—¶é—´
rate_limit[ip].append(now)

    start_time = time.time()
    trace_id = str(uuid.uuid4())[:8]  # ç®€æ´ä¸€ç‚¹

    if len(body.features) != 3:
        raise HTTPException(status_code=400, detail="âŒ è¾“å…¥ features å¿…é¡»åŒ…å« 3 ä¸ªæ•°å­—ã€‚")

    try:
        feature_tuple = tuple(body.features)
        prediction = cached_predict(feature_tuple)
        duration = (time.time() - start_time) * 1000
        logging.info(f"[trace:{trace_id}] {request.client.host} called /predict with input={body.features} â†’ output={prediction:.2f} | version={MODEL_VERSION} ({duration:.1f} ms)")
        return {
                "predicted_price": round(prediction, 2),
                "model_version": MODEL_VERSION,
                "trace_id": trace_id
        }
    except Exception as e:
        logging.error(f"Prediction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Prediction error: {str(e)}")

